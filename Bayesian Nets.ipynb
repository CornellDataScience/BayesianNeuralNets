{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Header'></a>\n",
    "\n",
    "# Introduction to Bayesian Neural Networks in Edward\n",
    "\n",
    "This tutorial is an introduction to Bayesian methods in Neural Networks, and should be an in-depth introduction to Bayesian inference methods and probabilistic modeling. Prior knowledge of basic probability theory is recommended before tackling all the concepts in this tutorial.\n",
    "\n",
    "As can be seen in the [table of contents](#TOC), this tutorial first sets up the problem of Bayesian linear regression along with an introduction to Bayesian infererence. Then we illustrate an implementation of BLR in Edward, and then move onto an introduction to approximate inference methods. From there, we tackle an implementation of a convolutional Neural Network (CNN) trained on the MNIST digit dataset. If you are already familiar with inference and Bayesian statistics, and are more interested in the workings of Edward with neural nets, you can go directly to the example implementation in [section 3](#BNN). \n",
    "\n",
    "As for the code in this notebook, the primary python library used to model BNNs is [Edward](http://edwardlib.org), a tensorflow library designed for probablistic modeling, and most importantly, approximate Bayesian inference and modeling. Make sure tensorflow, numpy, edward, as well as matplotlib, is properly installed before using this notebook.  \n",
    "  \n",
    "_(Note: If you have a pretty old computer, using tensorflow 1.5 or below maybe advised since a \"core dump: illegal instruction\" error may occur when importing edward. Otherwise ignore this warning)_\n",
    "\n",
    "Many thanks are given to Professor [Andrew Wilson](https://people.orie.cornell.edu/andrew/), who has both introduced and helped us understand many of the concepts written here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TOC'></a>\n",
    "\n",
    "# Table of contents\n",
    "\n",
    "### [1. Bayesian Linear Regression](#Bayesian Linear Regression)\n",
    "\n",
    "1.1 [Problem Setup: Simple Linear Regression](#BLR1)\n",
    "\n",
    "1.2 [Introduction to Bayesian Inference](#BLR2)\n",
    "\n",
    "1.3 [Implementing Bayesian Linear Regression](#BLR3)\n",
    "\n",
    "###   [2. Approximate Inference](#ApproxInf)\n",
    "\n",
    "2.1   [Limits of Exact Inference](#ApproxInf2)  \n",
    "\n",
    "2.1.1 [Optional Read: Bayesian Model Comparison](#addApprox)\n",
    "\n",
    "2.2 [Variational Inference](#ApproxInf3)\n",
    "\n",
    "2.2.1 [KL divergence](#ApproxInf4)\n",
    "\n",
    "2.2.2 [Evidence Lower Bound](#ApproxInf5)\n",
    "\n",
    "2.2 Markov Chain - Monte Carlo (MCMC) \n",
    "\n",
    "### [3. Bayesian Neural Network Example](#BNN)\n",
    "    \n",
    "3.1 [Bayesian Multilayer Perceptron](#BNN1)\n",
    "  \n",
    "3.2 [Bayesian Convolutional Neural Network](#BNN2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show=true; \n",
       "function code_toggle() {\n",
       " if (code_show){\n",
       " $('div.input').hide();\n",
       " } else {\n",
       " $('div.input').show();\n",
       " }\n",
       " code_show = !code_show\n",
       "} \n",
       "$( document ).ready(code_toggle);\n",
       "</script>\n",
       "Another Note/warning: The raw code for this IPython notebook is by default hidden for easier reading.\n",
       "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "Another Note/warning: The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Bayesian Linear Regression'></a>\n",
    "## 1. Bayesian Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BLR1'></a>\n",
    "### 1.1 Problem setup: simple linear regression\n",
    "To understand Bayesian inference and probabilistic modeling, let's first start with the simplest statistical model, a simple linear regression. Simple linear regression is a regression model that models an unknown relationship between two entities (denoted as $x$ and $y$) as a linear relationship such that $y_i$, an ith sample of $y$, can be written as $y_i = \\beta x_i + \\epsilon$. Notice here that we have used the language _ith sample_ here, indicating that we believe x and y to be some sort of distribution, from which samples, or data, is collected. \n",
    "\n",
    "Let's more rigorously set up the linear regression problem as done below:\n",
    "\n",
    "Given a dataset D with N data points:\n",
    "\n",
    "$$ D = \\{(x_i, y_i)\\}_{i=1}^{N}$$ \n",
    "\n",
    "where we have a collection of $x_i$ and $y_i$ pairs, find a linear relationship $\\beta$ that minimizes the following objective:  \n",
    "\n",
    "$$ \\sum_i^N (y_i - \\beta x_i)^2 $$  \n",
    "\n",
    "This formulation is commonly known as the __least squares problem__, or the best-fit line problem. If data points do not perfectly lie on a line, the least-squares problem proposes a method to find the best fit line that models a possible trend/relationship between the two distributions.\n",
    "\n",
    "We first show that the least-squares problem is mathematically equivalent to assuming the residual $y_i - \\beta x_i$, corresponding to the noise term $\\epsilon$, to be a _Gaussian white noise_. This noise has a mean at $\\mu = 0 $  with variance $\\sigma^2$, for all values of $i \\in N$. Linear regression therefore also treats $y$ as a Gaussian random variable, centered about $\\beta x$ and with variance $\\sigma^2$. We can represent this information with the following notation:\n",
    "$$ y \\sim Normal(\\beta x, \\sigma^2)$$\n",
    "\n",
    "To those interested in the math, one can derive the equivalence between the least squares objective and the Gaussian distributionby examining the __likelihood__ function for the Gaussian. Given the above expression of y, the likelihood function, a function that evaluates the probability of a given sample given our assumptions of the underlying mechanics, can be written as: \n",
    "\n",
    "$$ \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2} } exp(\\frac{-(y_i - \\beta x_i)^2}{\\sigma^2} )$$\n",
    "\n",
    "This is simply the product of probability densities evaluated at each sample in the dataset. We can simply multiply the probabilities together because we consider each data point as being (conditionally) independent from each other. Our objective in linear regression to find $\\beta$ such that this expression is maximized.\n",
    "\n",
    "Maximizing this likelihood function is equivalent to maximizing the log of it, since $x$ and $\\log x$ always increases/decreases together. Taking the log of the likelihood and simplifying the expression shows the implicit normality assumption in least-squares regression. For those interested, the derivation is done as the following:\n",
    "<br>\n",
    "$$argmax_\\beta \\hspace{0.25cm}\\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2} } exp(\\frac{-(y_i - \\beta x_i)^2}{\\sigma^2} )$$\n",
    "<br>\n",
    "$$\\iff argmax_\\beta \\hspace{0.25cm}\\ln{ \\big\\{ \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2} } exp(\\frac{-(y_i - \\beta x_i)^2}{\\sigma^2} \\big\\} }$$\n",
    "<br>\n",
    "$$\\iff argmax_\\beta \\hspace{0.25cm}\\sum_{i=1}^N \\big( -\\ln{\\sqrt{2\\pi\\sigma^2}} + \\frac{-(y_i - \\beta x_i)^2}{\\sigma^2} \\big\\}$$\n",
    "<br>\n",
    "$$\\iff argmax_\\beta\\hspace{0.25cm}\\sum_{i=1}^N\\frac{-(y_i - \\beta x_i)^2}{\\sigma^2}\\hspace{3.3cm}$$\n",
    "<br>\n",
    "$$\\iff  argmin_\\beta \\hspace{0.25cm}\\sum_{i=1}^N (y_i - \\beta x_i)^2\\hspace{3.7cm}$$\n",
    "\n",
    "<br><br>\n",
    "Optimizing the above expression to obtain an estimate of $\\beta$ above is an example of a __Maximum Likelihood Estimation(MLE)__ approach, or more generally speaking, a _frequentist_ approach. Frequentist approaches assume that there is enough data to conclude statistically significant conclusions about $\\beta$ given the dataset, and produce a single point solution for $\\beta$. This is often the approach taken for many conventional and widely adopted machine learning practices, but can be prone to overfitting when dataset is relatively small, or with increased complexity of the loss/likelihood functions - as is the case for neural networks - suffer from lack of interpretability.\n",
    "\n",
    "[back to top](#Header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='BLR2'></a>\n",
    "\n",
    "### 1.2 Introduction to Bayesian Inference\n",
    "\n",
    "_Bayesian_ approaches, on the other hand, avoid point estimations entirely. Rather, Bayesian method represent learned parameters - in the case for linear regression, the parameter $\\beta$ - as probability distributions rather than single optimal solutions.  This works well for many statistical modeling scenarios, where data insufficiently represents true relationships and functions due to noise or incorrect data, and thus may require us to represent _uncertainty_ in learned parameters. \n",
    "\n",
    "This information on parameter uncertainties also provides prediction uncertainties, and can be incredibly useful for fields like deep learning, where model uncertainty has traditionally been poorly understood due to complexity of neural networks.\n",
    "\n",
    "\n",
    "At the core of Bayesian methods lies the famous Bayes' Theorem, which can be written - in a more applicable form for probabilistic modeling - as the following:<br><br>\n",
    "\n",
    "$$ P(Parameters \\mid Data,Model) = \\frac{P(Data \\mid Parameters, Model) \\cdot P(Parameters\\mid Model)}{P(Data \\mid Model)} $$\n",
    "<br>\n",
    "<br>\n",
    "What makes Bayesian methods different is the addition of _prior belief_ to their models on the parameter being learned, which is represented as the second term on the numerator on the above expression as $P(Parameters \\mid Model)$. When size of collected samples used to train statistical model is small, this prior will have relatively significant influence on the behavior of the models, but with growing size of dataset diminish in its effects.\n",
    "\n",
    "The left side of this equation is what we aim to learn from the data, and what is called the _posterior distribution_ over the parameters, reflecting our beliefs on the parameter __after__ data has been observed. This posterior distribution can be further expanded as shown below:\n",
    "\n",
    "_Note: model and parameters all represented as $\\theta$ and data as D_\n",
    "<br><br><br>\n",
    "$$ Posterior \\hspace{1cm}\\iff\\hspace{1cm}P(\\theta \\mid D) = \\frac{P(D \\mid \\theta) \\cdot P(\\theta)}{\n",
    " P(D)} \\hspace{1cm}\\iff\\hspace{1cm} P(\\theta\\mid D) = \\frac{P(D \\mid \\theta) \\cdot P(\\theta)}{\n",
    "\\int P(D \\mid \\theta) \\cdot P(\\theta)\\hspace{0.1cm}d\\theta}$$\n",
    "<br><br>\n",
    "Notice that the first term on the numerator, $P(D \\mid \\theta)$ corresponds to the _likelihood_ of the data given model parameters, as we have already discussed in the previous section. Furthermore, the second term on the numerator corresponds to our prior beliefs on the parameter $\\theta$, as explained above. What is interesting is the denominator. The conversion of $P(D)$ into the integral is  the sum rule and the product rule of Bayes' Theorem, which respectively states that <br><br>\n",
    "\n",
    "$$P(x) \\iff \\int_{-\\infty}^\\infty{P(x \\cap y)\\hspace{0.1cm}dy} \\hspace{3cm}P(x,y)\\iff P(x\\mid y)\\cdot p(y)$$\n",
    "<br><br>\n",
    "The first one is called the sum rule because integrals are essentially summations, and is particularly important in Bayesian statistics because it allows functions with multiple unknown random variables to collapse to functions with less parameters, known as _marginal probability densities_. Here, the denominator for the posterior corresponds to a quantity called the _marginal likelihood_, where model parameters has been marginalized out using the sum rule. Therefore the above expression is also written as<br><br>\n",
    "$$ \\text{Posterior} \\iff \\frac{\\text{Likelihood} \\cdot \\text{Prior} }{\\text{Marginal Likelihood}} $$\n",
    "<br>\n",
    "  \n",
    "The process of evaluating this expression, hence learning the parameters of a proposed statistical model, is called __inference__, and is the most important part of Bayesian modeling. What is particularly noteworthy is that if the integral for the marginal likelihood is solvable, the posterior distribution can be directly calculated without any estimations. The difficulty in Bayesian methods arise from models where the marginal likelihood cannot be directly evaluated, as will be fully explained in [section 2](#ApproxInf)\n",
    "\n",
    "[back to top](#Header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BLR3'></a>\n",
    "### 1.3  Implementing Bayesian Linear Regression with Edward\n",
    "Now that we have an idea of how Bayesian methods work, let's set up this inference model for simple linear regression to understand how we can implement inference with Edward. To do so, we simply need to set up the data, the likelihood function and set a prior distribution that represents prior beliefs.\n",
    "\n",
    "__Data__:  \n",
    "The dataset will be a generated dataset, obtained by the following data-generating function:\n",
    "```python\n",
    "import numpy as np\n",
    "def toy_dataset(dataset_size, true_beta, noise_stdev):\n",
    "    # an array of randomly sampled integers\n",
    "    x = np.random.uniform(low=1, high = 20, size=dataset_size)\n",
    "    # target variable y is first initialized to true_beta * x\n",
    "    y = x * true_beta\n",
    "    # noise is a random normal with zero mean and stdev\n",
    "    noise = np.random.normal(scale = noise_stdev,size = dataset_size)\n",
    "    y += noise\n",
    "    return x,y\n",
    "```\n",
    "\n",
    "This dataset-generating function returns a uniformly sampled x, and y, which is the result of multiplying x by a constant and adding Gaussian noise. We then train test split a generated dataset with this code:\n",
    "```python\n",
    "# split data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(sample_data_x, sample_data_y, test_size = .2)\n",
    "```\n",
    "A plot of a sample dataset with 2000 data points - 1600 for train set and 400 for test -  made by this function is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30107b4f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "def toy_dataset(dataset_size, true_beta, noise_stdev):\n",
    "    # an array of randomly sampled integers\n",
    "    x = np.random.uniform(low=1, high = 20, size=dataset_size)\n",
    "    \n",
    "    # target variable y \n",
    "    y = x * true_beta\n",
    "    \n",
    "    # noise is a random normal with zero mean and stdev\n",
    "    noise = np.random.normal(scale = noise_stdev,size = dataset_size)\n",
    "    y += noise\n",
    "    \n",
    "    return x,y,true_beta, dataset_size\n",
    "\n",
    "sample_data_x, sample_data_y, true_beta,dataset_size = toy_dataset(20,10,1000)\n",
    "\n",
    "# split data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(sample_data_x, sample_data_y, test_size = .2)\n",
    "\n",
    "#plot the sample_data\n",
    "fig = plt.figure(figsize=(11,9))\n",
    "plt.scatter(sample_data_x,sample_data_y, c=\"black\")\n",
    "plt.title(\"Sample Data Scatterplot with true beta=\"+str(true_beta),fontsize=20)\n",
    "plt.xlabel(\"x\", fontsize=25)\n",
    "plt.ylabel(\"y\",fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Likelihood__:   \n",
    "In the simple linear regression setting, for a given value of $\\beta$, the likelihood of the data can be evaluated the following two ways:  \n",
    "If we use the likelihood of a Gaussian, we use $ \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2} } exp(\\frac{-(y_i - \\beta x_i)^2}{\\sigma^2} )$ and use some arbirary value of $\\sigma^2$ \n",
    "<br> \n",
    "\n",
    "  \n",
    "or using the equivalence of minimizing least squares with maximizing the likelihood of a Gaussian, we can also use $ -\\sum_i^N (y_i - \\beta x_i)^2 $  <br><br>\n",
    "\n",
    "__Prior__:   \n",
    "For this example, let's represent prior assumptions as a Gaussian with variance of 1 that is centered on a value that is different compared to the true $\\beta$\n",
    "\n",
    "We deliberately impose inaccurate prior beliefs, since this is more realistic, and will be illustrate the effectiveness of Bayesian methods even with incorrect priors. Also, keep in mind that if priors were really good and the parameter precisely known, there would be no need to do inference, or for that matter, model-based learning! \n",
    "\n",
    "<br>\n",
    "__Implementation:__\n",
    "\n",
    "Now we will actually implement this setup on Edward, as is illustrated in the code blocks shown below:<br><br>\n",
    "\n",
    "```python\n",
    "# Bayesian Linear Regression implementation in Edward\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "from edward.models import Normal\n",
    "import numpy as np\n",
    "```\n",
    "<br>\n",
    "Remember to have all of these libraries installed. Here we import Normal. However, there are many types of random variables (any probability distribution you can think of is most likely in there) supported by Edward, so feel free to play around with these. We will learn in the next section that there are only certain types of random variables that allow exact Bayesian inference. However, Edward is aimed to do approximate inference, and thus can support a much wider variety of random variables.<br><br>\n",
    "\n",
    "```python\n",
    "# defining the tensor that will hold the feature input\n",
    "X = tf.placeholder(tf.float32, shape=dataset_size)\n",
    "\n",
    "# beta will have inaccurate prior initializations since prior is a relatively narrow Gaussian on the wrong integer\n",
    "beta = Normal(loc=tf.constant(true_beta-5.0,shape=[1]),scale=tf.constant(100.0, shape=[1]))\n",
    "y = Normal(loc=X*beta, scale=tf.ones(dataset_size))\n",
    "\n",
    "# define the posterior\n",
    "qbeta = Normal(loc=tf.get_variable(\"qbeta/loc\", 1),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qbeta/scale\", 1)))\n",
    "\n",
    "```\n",
    "<br>\n",
    "The model used to instantiate the posterior determines the functional form of the _prior distribution_, which is the Gaussian/Normal distribution here. The loc, scale arguments each correspond to the mean $\\mu$ and standard deviation $\\sigma$ (and not the variance!) and can either be assigned manually as is done here or through some learned metafeatures derived from the data.<br><br>\n",
    "\n",
    "```Python\n",
    "# inference through minimizing the KL-divergence \n",
    "inference = ed.KLqp({beta:qbeta}, data ={X: train_x, y: train_y})\n",
    "inference.run(n_samples=5, n_iter=1000)\n",
    "inference.finalize()\n",
    "```\n",
    "<br>\n",
    "It should be noted that KLqp() is an approximate Inference method, as opposed to _exact inference_, and moreover, Edward is __not__ a library used for exact inference. This is not a setback. In fact, most frequent use cases of inference, as will be explained in the next section, will require approximate inference.\n",
    "\n",
    "```python\n",
    "# sample from the posterior and visualize\n",
    "fig = plt.figure(figsize=(11,9))\n",
    "post = qbeta.sample(100000).eval()\n",
    "plt.hist(post, bins = 500,normed=True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    " Shown below is the result of visualizing the posterior - through a normalized histogram constructed from samples - obtained by training the Bayesian Linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [100%] ██████████████████████████████ Elapsed: 70s | Loss: 11133788.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2e201a59e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bayesian Linear Regression implementation in Edward\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "from edward.models import Normal\n",
    "import numpy as np\n",
    "\n",
    "train_features = tf.placeholder(tf.float32, shape=train_x.shape)\n",
    "\n",
    "# beta will have inaccurate prior initializations since prior is a relatively narrow Gaussian on the wrong integer\n",
    "beta = Normal(loc=tf.constant(true_beta-10.0,shape=[1]),scale=tf.constant(1000.0, shape=[1]))\n",
    "target = Normal(loc=train_features*beta, scale=tf.ones(train_y.shape))\n",
    "\n",
    "# define the posterior\n",
    "qbeta = Normal(loc=tf.get_variable(\"qbeta/loc\", 1),\n",
    "            scale=tf.nn.softplus(tf.get_variable(\"qbeta/scale\", 1)))\n",
    "\n",
    "# inference through minimizing the KL-divergence \n",
    "inference = ed.KLqp({beta:qbeta}, data = {train_features: train_x, target: train_y})\n",
    "inference.run(n_samples=100, n_iter=4000)\n",
    "inference.finalize()\n",
    "\n",
    "\n",
    "# sample from the posterior and visualize\n",
    "fig = plt.figure(figsize=(11,9))\n",
    "plt.title(\"Histogram of Samples drawn from Posterior\")\n",
    "post = qbeta.sample(100000).eval()\n",
    "plt.hist(post, bins = 500,normed=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the inaccurate prior - centered at 5 with a fairly small standard deviation of 1 - does not cripple Bayesian methods at all, and that inferred posterior does a very good job approximating the true beta value. Let's compare to a point estimate of the true $\\beta$ value obtained by running sklearn's linear regression module. Since our posterior distribution is a Gaussian, the point estimate and the peak of the inferred posterior above should be roughly the same. The code block used is as follows:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression(fit_intercept=False)\n",
    "lm.fit(sample_data_x.reshape(-1,1), sample_data_y.reshape(-1,1))\n",
    "print(float(lm.coef_))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estmate of true beta is: -21.993722312477345\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2def8adcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression(fit_intercept=False)\n",
    "lm.fit(train_x.reshape(-1,1), train_y.reshape(-1,1))\n",
    "print(\"MLE estmate of true beta is: \"+str(float(lm.coef_)))\n",
    "\n",
    "plt.scatter(test_x, test_y,)\n",
    "temp = np.linspace(0,20,100)\n",
    "plt.plot(temp, lm.coef_*temp.reshape(-1,1), c='red')\n",
    "plt.title(\"MLE estimate of the true relationship\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should illustrate that inference is a comparable technique to MLE techniques. Let's see how comparable the two approaches are in terms of prediction accuracy. The way you do it is to first construct a predictive distribution from the posterior distribution. The prediction distribution can be obtained as the following:\n",
    "\n",
    "```python \n",
    "# first contruct the predictive distribution based on the inferred posterior distribution\n",
    "test_features = tf.placeholder(tf.float32, shape=test_x.shape)\n",
    "y_pred = Normal(loc=test_features * qbeta, scale=tf.ones(test_y.shape))\n",
    "```\n",
    "Here we define y_pred, which is the predictive distribution that uses the posterior qbeta as one of its inputs. Next, we compare it to the sklearn's linear regression model, and compare the mean squared error of both models:\n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "lm_pred = lm.predict(test_x.reshape(-1,1))\n",
    "print(\"Mean squared error on test data for Bayesian model:\")\n",
    "print(ed.evaluate('mean_squared_error', data={test_features: test_x, y_pred:test_y}))\n",
    "\n",
    "```\n",
    "The print statements produced are as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error on test data for Bayesian model:\n",
      "492093.56\n",
      "\n",
      "Mean squared error on test data for MLE model:\n",
      "492094.27905496664\n"
     ]
    }
   ],
   "source": [
    "# first contruct the predictive distribution based on the inferred posterior distribution\n",
    "test_features = tf.placeholder(tf.float32, shape=test_x.shape)\n",
    "y_pred = Normal(loc=test_features * qbeta, scale=tf.ones(test_y.shape))\n",
    "\n",
    "print(\"Mean squared error on test data for Bayesian model:\")\n",
    "print(ed.evaluate('mean_squared_error', data={test_features: test_x, y_pred:test_y}))\n",
    "\n",
    "# prediction accuracy for sklearn's linear regression module\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lm_pred = lm.predict(test_x.reshape(-1,1))\n",
    "\n",
    "print(\"\\n\"+\"Mean squared error on test data for MLE model:\")\n",
    "print(mean_squared_error(test_y.reshape(-1,1), lm_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Bayesian linear regression model does slightly better, although not too significantly. This is expected because the best value of beta for the training set will most likely be slightly off due to difference in training and test data due to in-sample noise. Bayesian models capture this uncertainty and therefore should have better generalizations in their predictions across different samples.\n",
    "\n",
    "\n",
    "[back to top](#Header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ApproxInf'></a>\n",
    "\n",
    "## 2. Approximate Bayesian Inference\n",
    "\n",
    "In the sections presented so far, we've shown how inference can work for a relatively straightforward case of simple linear regression with a Normal prior and have demonstrated a working implementation in Edward. However, we previously observed that Edward implements an iterative algorithm - with a function called KLqp - that approximates, rather than analytically solve for the posterior distribution. What is the approximation technique used/available on Edward and why do we approximate to start with? \n",
    "\n",
    "To answer these questions, we first explain when approximate inference is necessary or recommended, ands introduce two popular branches of such approximate inference techniques: __Variational Inference (VI)__ and __Markov-Chain-Monte-Carlo (MCMC)__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ApproxInf2'></a>\n",
    "\n",
    "### 2.1 Limits of Exact Inference\n",
    "\n",
    "Previously we explained that to do exact inference for the posterior, we evaluate the Bayes' formula:<br><br>\n",
    "\n",
    "$$\\frac{P(D \\mid \\theta) \\cdot P(\\theta)}{\n",
    "\\int P(D \\mid \\theta) \\cdot P(\\theta)\\hspace{0.1cm}d\\theta} \\hspace{.5cm} \\iff \\hspace{.75cm} \\frac{\\text{Likelihood}\\cdot{\\text{Prior}}}{\\text{Marginal Likelihood}}$$ \n",
    "<br>\n",
    "\n",
    "The critical component here is the integral in the denominator, or the _marginal likelihood_. The posterior can only be exactly evaluated if this integral is tractable, and therefore, solvable. This is true only for a very limited range of distribution families, which includes the exponential family - the Gaussian being the most prominent member -,the Gamma distribution, the Beta distribution and so on. However, for many complex hiearchical models, such as neural nets, the integral is in fact intractable and therefore cannot be exactly solved.\n",
    "\n",
    "There are two main approaches to tackling this limitation. One method employs an estimation of true posterior with a relatively easier-to-deal-with distribution, and is called variational inference. The other approach, Markov-chain-Monte-Carlo, samples directly from the posterior using Markov chains, evaluate the likelihood * prior over these samples and use the results instead of evaluating the integral. This section aims to expand on these methods, explain their relative strengths and weaknesses, so that we may expand on how to use them with Edward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='addApprox'></a>\n",
    "__Optional Read: Marginal likelihood and Bayesian Model Comparison __\n",
    "<br><br>\n",
    "While marginal likelihood typically impedes exact inference of the posterior, one may observe that the marginal likelihood can be regarded somewhat as a normalizing constant, and that in fact if we were simply interested in the shape of the posterior, one could use the relationship:\n",
    "\n",
    "$$ Posterior \\hspace{0.25cm} \\propto \\hspace{0.25cm} P(D \\mid \\theta) \\cdot P(\\theta)$$\n",
    "\n",
    "to get an unnormalized density proportional to the true posterior, avoiding the pesky integral entirely. So why would we use approximate inference?\n",
    "\n",
    "A large part is that the marginal likelihood, also called _model evidence_, is used for _model comparison_. This refers to comparing different types of models - between a linear regression and decision trees for example - or simply the same model with different hyperparameters on a given dataset, like kernel size, learning rate, and so on. Because models can all have very different values for their true marginal likelihoods, knowing the unnormalized density for the posterior of a given model makes comparison unreasonable.\n",
    "\n",
    "Bayesian model comparison uses the Bayes' Theorem again, where this time we try to evaluate the posterior probability of a model given data: \n",
    "<br><br>\n",
    "$$ P(Model \\mid Data) \\hspace{0.5cm}=\\hspace{0.5cm}\\frac{\\overbrace{P(Data \\mid Model)}^\\text{Model Evidence}\\cdot \\overbrace{P(Model)}^{\\text{Model Prior}}}{P(Data)} $$\n",
    "<br><br>\n",
    "Notice that to evaluate this we need the marginal likelihood, labelled in the above equation as 'model evidence', as previously stated. Once this posterior is evaluated for a model, we can compare it to posteriors of the other models. An example between two hypothetical models M1 and M2 is shown below:\n",
    "<br><br>\n",
    "\n",
    "$$ \\frac{P(M1 \\mid Data)}{P(M2\\mid Data)} \\hspace{0.5cm}=\\hspace{0.5cm}\\frac{P(Data \\mid M1)\\cdot P(M1)}{P(Data \\mid M2)\\cdot P(M2)}\\hspace{0.5cm}=\\hspace{0.5cm} \\underbrace{\\bigg(\\frac{P(Data \\mid M1)}{P(Data \\mid M2)}\\bigg)}_{\\text{Bayes Factor}}\\cdot \\underbrace{\\bigg(\\frac{P(M1)}{P(M2)}\\bigg)}_\\text{Prior Odds}$$\n",
    "\n",
    "From this expression, if we cannot reliably know a priori whether one model does better than the other, then the model comparison is a simple marginal likelihood ratio test, and one should prefer a model that produces significantly higher marginal likelihood/model evidence. This approach has the added advantage that models with higher complexity automatically is at a disadvantage due to smaller marginal likelihoods.  \n",
    "[back to top](#Header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ApproxInf3'></a>\n",
    "### 2.2 Variational Inference\n",
    "Variational inference is a very popular Bayesian method used for approximate inference, particularly for either intractable integrals, or tractable but high-dimensional distributions, which can be computationally expensive to deal with. Intuitively, VI approximates a complex distribution with distributions that are easier to deal with. If a distribution is not analytically solvable because of an intractable integral, one may use VI to approximate it with distributions with tractable solutions. Alternatively, one may also use VI for very high-dimensional but tractable distributions to reduce dimensionality.\n",
    "\n",
    "Shown below is an example of what VI attempts to do. P in this case is the _true density_ with a potentially intractable marginal likelihood. Q here is a _proposal distribution_, in the form a Gaussian distribution, that approximates P. While they may not be exact, a close enough Q can yield very similar and reliable results in inference and predictions. \n",
    "\n",
    "![image](http://davmre.github.io/blog/assets/images/variational_approximation.png)\n",
    "\n",
    "This procedure is typically done by minimizing a distance measure between two distributions known as __KL divergence__ using stochastic gradient descent(SGD). While variational inference as a method has existed for quite some time, its newfound popularity is due to advances in SGD, as well as advances in calculating what is known as Evidence Lower Bound, which provides an estimate of the lower bound for the marginal likelihood, which has allowed variational inference to be effective in a much wider family of distributions.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ApproxInf4'></a>\n",
    "__ 2.2.1 KL divergence__  \n",
    "\n",
    "KL divergence, or the Kullback-Leibler divergence, is a distance measure between two density functions that has the following form:\n",
    "\n",
    "$$D_{kl}(P\\mid\\mid Q)= -\\int_{i} P(i) \\log{\\frac{Q(i)}{P(i)}} $$\n",
    "\n",
    "However, in practice, sampling is used to compare the two distributions, since this is much more computationally tractable, and therefore the KL divergence formula more useful is the following:\n",
    "\n",
    "$$D_{kl}(P\\mid\\mid Q)= -\\sum_{i} P(i) \\log{\\frac{Q(i)}{P(i)}} \\hspace{1cm} \\text{and} \\hspace{1cm} D_{kl}(Q\\mid\\mid P)= -\\sum_{i} Q(i) \\log{\\frac{P(i)}{Q(i)}}  $$\n",
    "\n",
    "Notice that this distance measure is not necessarily symmetric, that is:\n",
    "<br><br>\n",
    "\n",
    "$$D_{kl}(P\\mid\\mid Q) \\neq D_{kl}(Q\\mid\\mid P)$$\n",
    "<br><br>\n",
    "This is an important distinction, as the two metrics will differ in the outcome they produce. Minimizing $D_{KL}(P \\mid \\mid Q)$ encourages Q to be small in areas that P is small, since P is in the denominator for the log expression. Whereas minimizing $D_{KL}(Q \\mid \\mid P)$ drives Q to be large where P is large, and may even drive Q towards the largest peak of P. A visualization is shown below:\n",
    "![image](https://benmoran.files.wordpress.com/2012/07/wpid-kloptim.png?w=500)\n",
    "\n",
    "Here the red line is P, the true distribution. Blue line a result of minimizing $D_{KL}(Q \\mid \\mid P)$, and green the result of minimizing $D_{KL}(P \\mid \\mid Q)$. Blue line is driven towards the largest peak as expected, where as green line approximates it more broadly over a region of relatively high mass. Each approximation can have its own advantages and disadvantages. However  $D_{KL}(Q \\mid \\mid P)$ can be problematic when the highest peak is very sharp, and does not necessarily represent the majority of the true distribution, such as the following distribution:<br><br>\n",
    "\n",
    "\n",
    "![image](https://www.researchgate.net/profile/M_Uleysky/publication/304989177/figure/fig4/AS:613990153596929@1523398068151/The-distribution-of-a-number-of-long-w-flights-with-T-f-10-3-over-their-mean-zonal.png)\n",
    "<br><br>\n",
    "\n",
    "Here the sharp peak may not necessarily be a good representation of the entire distribution, and will lead to inaccurate representation of the posterior after variational inference. One may need to consider what the general shape of the posterior is before employing variational inference. One way one could do this is to first obtain the shape of the loss surface by calculating only the numerator in the posterior expression. This avoids the problem of calculating the marginal likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ApproxInf5'></a>\n",
    "__ 2.2.2 How to minimize KL divergence / Evidence Lower Bound(ELBO)__\n",
    "\n",
    "In this section we show minimizing the KL divergence is equivalent to maximizing what is known as the __Evidence Lower Bound__, or ELBO. This optimization often takes form of a gradient descent operation. The equivalence between KL divergence and ELBO is shown below.\n",
    "\n",
    "First of all, if we are to be more rigorous with our notations, $D_{kl}(Q \\mid\\mid P)$ should be actually written as $ D_{kl}(q(y) \\mid\\mid p(y\\mid x) )$ <br><br>\n",
    "\n",
    "We expand upon the KL divergence expression as follows:\n",
    "$$ $$\n",
    "$$ D_{kl}(q(y) \\mid\\mid p(y\\mid x) ) = -\\int_y{\\log{p(y \\mid x)}} $$\n",
    "\n",
    "Recall that $ P(y \\mid x)$ can be written as  $\\frac{p(y,z)}{} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Markov-Chain Monte Carlo\n",
    "TODO\n",
    "\n",
    "__ Metropolis - Hastings__  \n",
    "__ Gibbs sampling __    \n",
    "__ Slice Sampling __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"BNN\"></a>\n",
    "## 3. Bayesian Neural Network Implementation in Edward\n",
    "\n",
    "Let's dive into an implementation of Bayesian neural networks. Edward is the main library we're going to use here. Edward is a probablistic programming library that sits on top of tensorflow. This means it benefits from Tensorflow's graph construction and optimization, as well as ease of GPU acceleration in inference. It also interacts well with Keras's layers, an advantage that we will also explore in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='BNN1'></a>\n",
    "### 3.1 Bayesian Multilayer Perceptron\n",
    "\n",
    "Let's start with a simple Feedforward network, also known as a Multilayer Perceptron (MLP). Here we will use the MNIST digit recognition data. The data extraction is done with the following code. \n",
    "\n",
    "```python\n",
    "# use Keras and Edward\n",
    "from edward.models import Normal, OneHotCategorical\n",
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# download the MNIST dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot = True)\n",
    "\n",
    "# assign train test data with corresponding features(x) and labels(y)\n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels\n",
    "\n",
    "# assign the dimensions of the training data\n",
    "N, D = x_train.shape\n",
    "```\n",
    "Notice that we use both Edward and tensorflow. We also import Normal and OneHotCategorical modules, which will be used to represent parameters and neural network output respectively. \n",
    "<br><br>\n",
    "\n",
    "__1. Defining the model __\n",
    "\n",
    "One way to use Edward is to first define the model explicitly. In other words, this means defining the workflow of the model given certain probablistic inputs. This is similar to how probablistic modeling works. Typically a graphical model is constructed first to understand the hierarchy and model construction. Below is a 2-hidden-layer fully connected neural network.\n",
    "\n",
    "```python\n",
    "# define a neural network with two hidden layers that uses the tanh activation\n",
    "\n",
    "def neural_network_classifier(x, W_0, W_1, W_2, b_0,b_1, b_2 ):\n",
    "    # first hidden layer\n",
    "    h = tf.nn.relu(tf.matmul(x, W_0) + b_0)\n",
    "    \n",
    "    # second hidden layer\n",
    "    h = tf.nn.relu(tf.matmul(h, W_1) + b_1)\n",
    "    \n",
    "    # final output\n",
    "    h = tf.nn.softmax(tf.matmul(h, W_2) + b_2)\n",
    "    \n",
    "    return h\n",
    "    \n",
    "# define the inputs and how y is derived\n",
    "x = tf.placeholder(tf.float32,[N,D])\n",
    "y = OneHotCategorical(probs = neural_network_classifier(x, W_0,W_1, W_2, b_0, b_1, b_2))\n",
    "```\n",
    "\n",
    "As explained above, we first defined the model construction. In our case, this neural network has two hidden layers, use rectified linear units (ReLU) which simply does a  $max(0,W\\cdot x+b) for each layer. The last output does a softmax transformation, since we want the neural network to output a probability-like value for each of the classes. Our problem is the MNIST digid recognition problem, therefore we define the output to have size 10 for any input. We then define how y should be constructed as a one-hot categorical from this neural network output.<br><br>\n",
    "\n",
    "__2. Defining the prior distribution __\n",
    "\n",
    "Once we've defined a model that follows a certain hierarchy/workflow, we can define each of the inputs as a __random variable__. This is the key feature of Edward:it allows for probablistic modeling by defining input parameters to a model as stochastic variables. These random variables represent our prior beliefes about these parameters. Here we define them to be a multivariate standard normal, with the dimensions of the defined normal variables matching those in the predefined model above.\n",
    "\n",
    "```python\n",
    "W_0 = Normal(loc=tf.zeros([D, 100]), scale=tf.ones([D, 100]))\n",
    "W_1 = Normal(loc=tf.zeros([100, 100]), scale=tf.ones([100, 100]))\n",
    "W_2 = Normal(loc=tf.zeros([100, 10]), scale=tf.ones([100, 10]))\n",
    "b_0 = Normal(loc=tf.zeros(100), scale=tf.ones(100))\n",
    "b_1 = Normal(loc=tf.zeros(100), scale=tf.ones(100))\n",
    "b_2 = Normal(loc=tf.zeros(10), scale=tf.ones(10))\n",
    "```\n",
    "\n",
    "It should be noted that one does not necessarily have to choose the normal distribution for these priors. One may choose to use any random variable available in Edward's model API ([link](http://edwardlib.org/api/model)) as these priors. Once we've defined our prior assumptions using these, we can now proceed to define estimation variables. Remember from [2. Approximate Bayesian Inference](#ApproxInf) section from this notebook that the posterior, $ P(\\theta \\mid \\text{Data}) $, must always be approximated save for a very few restrictive cases where the integral for the marginal likelihood is in fact tractable.\n",
    "\n",
    "In our case we have a weight and bias variable for each layer. We need to define estimation random variables for these. This is similar to the Bayesian linear regression model, where we define the prior distribution over the parameters, and then define the form of the proposed distribution for each of them, as the following:\n",
    "\n",
    "```python\n",
    "# INFERENCE\n",
    "with tf.variable_scope(\"posterior\"):\n",
    "    # First hidden layer weight and bias\n",
    "    with tf.variable_scope(\"qW_0\"):\n",
    "        loc = tf.get_variable(\"loc\", [D, 100])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [D, 100]))\n",
    "        qW_0 = Normal(loc=loc, scale=scale)\n",
    "    with tf.variable_scope(\"qb_0\"):\n",
    "        loc = tf.get_variable(\"loc\", [100])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [100]))\n",
    "        qb_0 = Normal(loc=loc, scale=scale)\n",
    "    \n",
    "    # Second hidden layer weight and bias\n",
    "    with tf.variable_scope(\"qW_1\"):\n",
    "        loc = tf.get_variable(\"loc\", [100, 100])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [100, 100]))\n",
    "        qW_1 = Normal(loc=loc, scale=scale)\n",
    "    with tf.variable_scope(\"qb_1\"):\n",
    "        loc = tf.get_variable(\"loc\", [100])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [100]))\n",
    "        qb_1 = Normal(loc=loc, scale=scale)\n",
    "        \n",
    "    # Output Softmax layer weight and bias\n",
    "    with tf.variable_scope(\"qW_2\"):\n",
    "        loc = tf.get_variable(\"loc\", [100, 10])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [100, 10]))\n",
    "        qW_2 = Normal(loc=loc, scale=scale)\n",
    "    with tf.variable_scope(\"qb_2\"):\n",
    "        loc = tf.get_variable(\"loc\", [10])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [10]))\n",
    "        qb_2 = Normal(loc=loc, scale=scale)\n",
    "```\n",
    "\n",
    "All the with statements here facilitate the tensorflow program to differentiate different variable names. For clean code, we have defined each argument of the Normal() objects outside the object first. While the estimation variables here have the exact same dimensions as their estimated variable counterparts, this does not have to be the case. They can be lower dimensions to reduce computational complexity. They must however match the dimensions of one another to make sure the matrix multiplications in the model function __neural_network_classifier()__ still works.\n",
    "\n",
    "Once you have defined all relevant proposal and prior distribution forms, you simply define what approximate inference method to use (in this case variational inference using KL-divergence minimization). \n",
    "\n",
    "```python\n",
    "# use KL divergence minimization\n",
    "inference = ed.KLqp({W_0: qW_0, b_0: qb_0,\n",
    "                       W_1: qW_1, b_1: qb_1,\n",
    "                       W_2: qW_2, b_2: qb_2}, data={x: x_train, y: y_train})\n",
    "# run inference\n",
    "inference.run(n_samples=5, n_iter=1000)\n",
    "inference.finalize()\n",
    "```\n",
    "\n",
    "Again notice that we are using KLqp and not KLpq. This is because the former typically is computationally faster. One may choose to change the approximation method to see if it offers any advantages. Running this code will, just like it did for Bayesian Linear regression, show a progress bar showing the variational inference iterations at work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# download the MNIST dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)\n",
    "# assign train test data with corresponding features(x) and labels(y)\n",
    "x_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "\n",
    "x_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use Keras and Edward\n",
    "from edward.models import Normal, OneHotCategorical\n",
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# define a neural network with two hidden layers that uses the tanh activation\n",
    "\n",
    "def neural_network_classifier(x, W_0, W_1, W_2, b_0,b_1, b_2 ):\n",
    "    # first hidden layer\n",
    "    h = tf.nn.relu(tf.matmul(x, W_0) + b_0)\n",
    "    \n",
    "    # second hidden layer\n",
    "    h = tf.nn.relu(tf.matmul(h, W_1) + b_1)\n",
    "    \n",
    "    # final output\n",
    "    h = tf.nn.softmax(tf.matmul(h, W_2) + b_2)\n",
    "    \n",
    "    return h\n",
    "\n",
    "N, D = x_train.shape\n",
    "\n",
    "W_0 = Normal(loc=tf.zeros([D, 100]), scale=tf.ones([D, 100]))\n",
    "W_1 = Normal(loc=tf.zeros([100, 100]), scale=tf.ones([100, 100]))\n",
    "W_2 = Normal(loc=tf.zeros([100, 10]), scale=tf.ones([100, 10]))\n",
    "b_0 = Normal(loc=tf.zeros(100), scale=tf.ones(100))\n",
    "b_1 = Normal(loc=tf.zeros(100), scale=tf.ones(100))\n",
    "b_2 = Normal(loc=tf.zeros(10), scale=tf.ones(10))\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32,[N,D])\n",
    "y = OneHotCategorical(neural_network_classifier(x, W_0,W_1, W_2, b_0, b_1, b_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/deeplearning_1/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/distributions/python/ops/onehot_categorical.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "2000/2000 [100%] ██████████████████████████████ Elapsed: 1415s | Loss: 87879.281\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE using VI\n",
    "with tf.variable_scope(\"posterior\"):\n",
    "    with tf.variable_scope(\"qW_0\"):\n",
    "        loc = tf.get_variable(\"loc\", [D,100])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [D,100]))\n",
    "        qW_0 = Normal(loc=loc, scale=scale)\n",
    "    with tf.variable_scope(\"qW_1\"):\n",
    "        loc = tf.get_variable(\"loc\", [100, 100])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [100, 100]))\n",
    "        qW_1 = Normal(loc=loc, scale=scale)\n",
    "    with tf.variable_scope(\"qW_2\"):\n",
    "        loc = tf.get_variable(\"loc\", [100, 10])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [100, 10]))\n",
    "        qW_2 = Normal(loc=loc, scale=scale)\n",
    "    with tf.variable_scope(\"qb_0\"):\n",
    "        loc = tf.get_variable(\"loc\", [100])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [100]))\n",
    "        qb_0 = Normal(loc=loc, scale=scale)\n",
    "    with tf.variable_scope(\"qb_1\"):\n",
    "        loc = tf.get_variable(\"loc\", [100])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [100]))\n",
    "        qb_1 = Normal(loc=loc, scale=scale)\n",
    "    with tf.variable_scope(\"qb_2\"):\n",
    "        loc = tf.get_variable(\"loc\", [10])\n",
    "        scale = tf.nn.softplus(tf.get_variable(\"scale\", [10]))\n",
    "        qb_2 = Normal(loc=loc, scale=scale)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "        \n",
    "inference = ed.KLqp({W_0: qW_0, b_0: qb_0,\n",
    "                       W_1: qW_1, b_1: qb_1,\n",
    "                       W_2: qW_2, b_2: qb_2}, data={x: x_train, y: y_train})\n",
    "\n",
    "inference.initialize(logdir='log')          \n",
    "inference.run(n_samples=100, n_iter=2000)\n",
    "inference.finalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the progress bar is full, the posterior approximation has been complete and now we can move onto prediction. This part is almost identical to the part in the Bayesian linear regression example. We first set up the input placeholders and prediction random variable.\n",
    "\n",
    "```python\n",
    "test_features = tf.placeholder(tf.float32, shape=x_test.shape)\n",
    "y_pred = OneHotCategorical(probs=neural_network_classifier(test_features, qW_0, qW_1, qW_2, qb_0,qb_1, qb_2 ))\n",
    "\n",
    "```\n",
    "\n",
    "Here y_pred is a random variable defined by probabilities vector over the 10 classes. We can then evaluate the test accuracy by running ed.evaluate, as shown below:\n",
    "```python\n",
    "print(\"Accuracy on test MNIST data for the Bayesian :\")\n",
    "print(ed.evaluate('categorical_accuracy', data={test_features: x_test, y_pred:y_pred}))\n",
    "```\n",
    "\n",
    "The output is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test MNIST data for the Bayesian :\n",
      "0.9579\n"
     ]
    }
   ],
   "source": [
    "test_features = tf.placeholder(tf.float32, shape=x_test.shape)\n",
    "y_pred = OneHotCategorical(neural_network_classifier(test_features, qW_0, qW_1, qW_2, qb_0,qb_1, qb_2 ))\n",
    "print(\"Accuracy on test MNIST data for the Bayesian :\")\n",
    "print(ed.evaluate('categorical_accuracy', data={test_features: x_test, y_pred:y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it takes some computation to actually calculate the accuracy. This is because to make a prediction, one must collapse the predictive distribution to a single value. In most cases, this is equivalent to taking the expectation of the predictive distribution. In practive samples are drawn from the predictive distribution and then averaged. We see that this relatively simple setup manages to get a 100% accuracy on the MNIST dataset, which shows the effectiveness of the Bayesian framework.\n",
    "\n",
    "[Back to top](#Header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from edward.models import Empirical\n",
    "\n",
    "with tf.variable_scope(\"posterior_mcmc\"):\n",
    "    qW_0 = Empirical(params = tf.get_variable(\"qW_0/params\", [200,784,400]))\n",
    "    qW_1 = Empirical(params = tf.get_variable(\"qW_1/params\", [200,400,400]))\n",
    "    qW_2 = Empirical(params = tf.get_variable(\"qW_2/params\", [200,400,10]))\n",
    "    qb_0 = Empirical(params = tf.get_variable(\"qb_0/params\", [200,400]))\n",
    "    qb_1 = Empirical(params = tf.get_variable(\"qb_1/params\", [200,400]))\n",
    "    qb_2 = Empirical(params = tf.get_variable(\"qb_2/params\", [200,10]))\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "inference_hmc = ed.HMC({W_0: qW_0, b_0: qb_0,\n",
    "                       W_1: qW_1, b_1: qb_1,\n",
    "                       W_2: qW_2, b_2: qb_2}, data={x: x_train, y: y_train})\n",
    "\n",
    "inference_hmc.run(step_size=0.01, n_steps = 10000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "test_features = tf.placeholder(tf.float32, shape=x_test.shape)\n",
    "y_pred = OneHotCategorical(neural_network_classifier(test_features, qW_0, qW_1, qW_2, qb_0,qb_1, qb_2 ))\n",
    "print(\"Accuracy on test MNIST data for the Bayesian :\")\n",
    "print(ed.evaluate('categorical_accuracy', data={test_features: x_test, y_pred:y_test}))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
